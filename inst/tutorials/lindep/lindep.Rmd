---
title: "No Perfect Collinearity"
output: learnr::tutorial
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE)
turnout <- read.csv("http://qss.princeton.press/student-files/INTRO/turnout.csv")
```


## Motivating Question

### What is the association between race and income?

*Building a regression model*

Let's say we want to explore the relationship between race and income, where the people in our sample take on the values white, Black, Asian, and Hispanic for race. We can write this as:

$Income_i = \alpha + \beta*race_i + \epsilon$

However, race is not a numeric variable. This complicates our regression equation because what exactly is a 1-unit change in race? Sure, we could assign numeric values to each racial category in our data (e.g., white = 1, Black = 2,  Hispanic = 3, Asian = 4), but we would have no reason to assume that the change in income would be linear as you change in race by units. Why should the difference in income between white and Black individuals be estimated as the same difference between Black and Hispanic individuals?


```{r quiz1}
quiz(
  question("In a linear regression, when you have categorical independent variables, what should you typically do?",
    answer("Treat as if a numeric variable"),
    answer("Convert to a set of dummy or indicator variables", correct = TRUE),
    answer("Give up")))
```


## Working with categorical independent variables

### Let's build some data

*Build a matrix with dummy variables for each race*

Run the code below and see what is in X.
```{r exercise1, exercise = TRUE, exercise.lines=21}
## Dummy variable example
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
X <- cbind(white, asian, black, hispanic)
```



*Let's build toward a regression model*

Let's create a `Y` variable representing our outcome for income. Let's also add an intercept to our `X` matrix. Take a look into our new X.

```{r setupA}
## Dummy variable example
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
X <- cbind(white, asian, black, hispanic)
``` 

```{r ex2, exercise=TRUE, exercise.lines = 6, exercise.setup = "setupA"}
## Dependent variable
Y <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))

X <- cbind(1, X)
```


*Let's now apply the formula $(X'X)^{-1}X'Y$ to estimate our coefficients.*

Assign the output to an object called `betahat`
```{r setupB}
## Dummy variable example
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
X <- cbind(white, asian, black, hispanic)
Y <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))

X <- cbind(1, X)
```

```{r ex3, exercise = TRUE, exercise.lines = 4, exercise.setup="setupB"}

```

```{r ex3-solution}
betahat <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat
```

*Diagnosing the error*

```{r quiz2}
quiz(
  question("Why do we get an error message here?",
    answer("We have linear dependencies in our columns.", correct = TRUE),
    answer("We cannot invert the $X'X$ matrix.", correct = TRUE),
    answer("We cannot use dummy variables in linear regression.")))
```


## Formalizing Linear Dependence

### Why were our dummy variables linear dependent?

If we inspect `X` we can see that taking each column $\mathbf{x_1} - \mathbf{x_2} - \mathbf{x_3}-\mathbf{x_4}=0$. There is a linear relationship between the variables.

Formally, a set of vectors (e.g.,$\mathbf{x_1}, \mathbf{x_2}, ...\mathbf{x_k}$) is linearly independent if the equation $\mathbf{x_1}*a_1 + \mathbf{x_2}*a_2 +... + \mathbf{x_k}*a_k= 0$ only in the trivial case where $a_1$ and $a_2$ through $a_k$ are 0. A set of vectors has a linearly dependent relation if there is a solution $\mathbf{x_1}*a_1 + \mathbf{x_2}*a_2 +... + \mathbf{x_k}*a_k = 0$ where not all $a_1, a_2$ through $a_k$ are 0.


For OLS, we must assume no perfect collinearity. 

  - No independent variable is constant 
  - No exactly linear relationships among the independent variables
  - The rank of X is $k$ where the rank of a matrix is the maximum number of linearly independent columns.
  
As discussed in section 3.5 of the course notes, a square matrix is only invertible if its columns are linearly independent. In OLS, in order to estimate unique solutions for $\hat \beta$, we need to invert $(X'X)^{-1}$. When we have perfect collinearity, we cannot do this.
  
*Note the linear dependence in X*

```{r setupB2}
## Dummy variable example
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
X <- cbind(white, asian, black, hispanic)
Y <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))

X <- cbind(1, X)
```


```{r ex4, exercise = T, exercise.lines = 20, exercise.setup = "setupB2"}
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
X <- cbind(1, white, asian, black, hispanic)

X[, 1] - X[, 2] - X[, 3] - X[, 4] - X[,5]
```


*Try to take $(X'X)^{-1}$*

```{r ex5, exercise = T, exercise.lines = 4, exercise.setup = "setupB2"}
solve(t(X) %*% X)
```



### How do we correct this?

To address this, we are going to drop one of the categorical variables when we run the regression. Consequently, our coefficients will now be interpreted as differences between this reference category (the category left out, e.g., white) and the particular group (e.g., white vs. Asian or white vs. Black or white vs. Hispanic).

*Redefine `X` removing the white column, and calculate $(X'X)^{-1}X'Y$*

```{r ex6, exercise = T, exercise.lines = 5, exercise.setup = "setupB2"}
X <- cbind(1, asian, black, hispanic)
betahat <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat 
```


*Check this with the `lm()` function*
```{r ex7, exercise = T, exercise.lines = 5, exercise.setup = "setupB2"}
summary(lm(Y ~ asian + black + hispanic))
```


### R Danger zone

In R and many other statistical softwares, the regression function will forcibly drop one of your variables if it encounters this type of linear dependence. See below when we include all four race dummies in the model.

*Check what happens with the `lm()` function*

```{r ex8, exercise = T, exercise.lines = 5, exercise.setup = "setupB2"}
summary(lm(Y ~ white + asian + black + hispanic))
```

An alternative way to enter categorical variables in a regression is to let the function create the dummy variables for you using `factor(var, levels = )` to make sure R knows it is a factor variables.

```{r setupC}
## Dummy variable example
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
Y <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))
```

```{r ex9, exercise = T, exercise.lines = 15, exercise.setup = "setupC"}
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")
Y <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))

resprace <- factor(resprace, levels = c("white", "asian", "black", "hispanic"))
summary(lm(Y ~ resprace))
```


## Other examples of breaking the no perfect collinearity rule

Recall, for OLS, we must assume no perfect collinearity. 

  - No independent variable is constant 
  - No exactly linear relationships among the independent variables
  - The rank of X is $k$ where the rank of a matrix is the maximum number of linearly independent columns.
  
Let's say we wanted to control for age, but all of our sample was 18 years old. Let's try to add this to the `X` matrix.

*Redefine `X` adding age column, and calculate $(X'X)^{-1}X'Y$*

```{r setupD}
## Dummy variable example
resprace <- c("white", "white", "asian", "black", "hispanic",
              "hispanic", "hispanic", "asian", "white", "black", 
              "black", "black", "asian", "asian", "white", "white")

## "Dummy variables"
white <- rep(0, length(resprace))
white[resprace == "white"] <- 1
asian <- rep(0, length(resprace))
asian[resprace == "asian"] <- 1
black <- rep(0, length(resprace))
black[resprace == "black"] <- 1
hispanic <- rep(0, length(resprace))
hispanic[resprace == "hispanic"] <- 1

## Matrix
Y <- cbind(c(10, 11, 9, 8, 9, 7, 7, 13, 12, 11, 8, 7, 4, 13, 8, 7))
```

```{r ex10, exercise = T, exercise.lines = 5, exercise.setup = "setupD"}
age <- rep(18, length(resprace))
X <- cbind(1, asian, black, hispanic, age)
betahat <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat 

```

Let's visit the Florida example. Let's say we had two `Perot96` variables-- one the raw votes and one where votes were multiplied by 1000 to adjust the order of magnitude.

*Regress Buchanan's votes on Perot and Perot adjusted values*

```{r ex11, exercise = T, exercise.lines =10}
florida <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv")

florida$perotadjusted <- florida$Perot96 * 1000

Y <- florida$Buchanan00
X <- cbind(1, florida$Perot96, florida$perotadjusted)
betahat <- solve(t(X) %*% X) %*% t(X) %*% Y
betahat 
```

